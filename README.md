# Credit-card-fraud-detection Summary

1. This entire notebook has been created on Google Colab and language used is python 
2. Answers to the questions have been answered as I move ahead in the notebook, and a seperate pdf file has also been attached to review the same 
3. Import Libraries - Initially as the first step all the essential libraries like numpy, pandas, seaborn, sklearn's different modules, imblearn and xgboost have been imported 
4. Loading the data - Loaded the data using a pandas function read_json and stored it in a dataframe 'trans_df' 
5. Analyzed the data types, statistics of different features and changed data types wherever required. Found out about missing values in the data, and pre-processed it ahead 
6. Data Visualisation - Mapped different features into two buckets - discrete and continuous. Plotted and studied different distributions on the continuous data. An important finding that the target class is highly imbalanced 
7. Data Wrangling - To find the reversal transactions, made use of the REVERSAL identifier tage in the transactionType column and leveraged count and sum functions to get the asked results. For the multiswipe transactions, data was grouped based on customerId and with consecutive time point of the next transaction in the queue it was decided if the transaction is multiswipe or not 
8. Data Cleaning (Handling Missing values) - The df.is_na function did not detect missing values intially as it does not take empty string format into consideration. So, replaced the empty strings with NaN value in the dataframe. Dropped 6 columns as they were null and empty. Dropped data points from acqCountry  and merchantCountryCode as they were a part of constant column with negligible standard deviation i.e most values in these two columns indicate a constant value 'US'. Dropped data points from other columns due to lack of domain knowledge and taking into account that they identified a minimal subset (less than 0.5%) in the dataset 
9. Feature Engineering - Before Model Building, feature engineering is performed to reduce the dimensions and drop features that will not affect the result quantitatively. Dropped columns with datetime data type, and columns with huge subset of categories. After finding the correlation between different features, few features were dropped having a high correlation. Encoded data types where necessary to perform feature selection next 
10. Feature Selection - Performed ExtraTreesRegressor and SelectKBest to identify important features based on a score metric and p value. Dropped features that did not have a significant score. 
11. Model Building - To get the data ready, over sampling using SMOTETOmek was performed to tackle the class imbalance that was discovered earlier. Undersampling did not work in this case as most of the data was getting lost which resulted in very inaccurate results. Different classification models were built - Logistic regression, Decision Tree, Random Forest and XGBoost. Tree based models outperformed the rest, and Random Forest gave the best results in terms of all the metrics - precision, recall, accuracy and roc-auc score which was cross validated and it performed as good on different data folds too
